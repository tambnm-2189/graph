{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### no auto gradient.\n",
    "def init_xavior_uniform(num_outs, num_ins = None, gain = 1.):\n",
    "    if num_ins is None:\n",
    "        var = 1/num_outs\n",
    "        size = (num_outs,)\n",
    "\n",
    "    else:\n",
    "        var = 6/(num_ins + num_outs)\n",
    "        size = (num_ins, num_outs)\n",
    "\n",
    "    factor = gain*np.sqrt(var)\n",
    "    return np.random.uniform(low = -factor, \n",
    "                             high = factor, \n",
    "                             size = size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self, data = None):\n",
    "        self.data = data\n",
    "        self.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def forward(self, x):pass\n",
    "    \n",
    "    def __call__(self, x): return self.forward(x)\n",
    "    \n",
    "    @property\n",
    "    def params(cls):\n",
    "        return (getattr(cls, i) for i in cls.__dict__.keys() if isinstance(getattr(cls, i), Parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, num_ins, num_outs):\n",
    "        super().__init__()\n",
    "        self.weight = Parameters(init_xavior_uniform(num_outs, num_ins))\n",
    "        self.bias = Parameters(init_xavior_uniform(num_outs = num_outs))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # X: (batch, num_ins)\n",
    "        z = np.dot(x, self.weight.data) + self.bias.data\n",
    "        \n",
    "        ## record all intermediate that used for backward\n",
    "        back = {'x': x}\n",
    "        return z, back\n",
    "    \n",
    "    def backward(self, dout, back):\n",
    "        ## return gradients for update\n",
    "        x = back['x']\n",
    "        self.bias.grad = np.sum(dout, axis = 0)\n",
    "        self.weight.grad = np.dot(x.T, dout)\n",
    "        print(self.weight.grad.shape)\n",
    "        dx = np.dot(dout, self.weight.data.T)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "(10, 15)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "##TESTING\n",
    "x = np.random.randn(5, 10)\n",
    "l1 = Linear(10, 15)\n",
    "weight, bias = l1.weight.data, l1.bias.data\n",
    "\n",
    "l2 = nn.Linear(in_features= 10, out_features= 15)\n",
    "l2.weight = torch.nn.Parameter(torch.tensor(weight.T))\n",
    "l2.bias = torch.nn.Parameter(torch.tensor(bias))\n",
    "\n",
    "#FORWARD TESTING\n",
    "out1, back = l1(x) \n",
    "\n",
    "x2 = torch.tensor(x, requires_grad = True)\n",
    "x2.retain_grad()\n",
    "out2 = l2(x2)\n",
    "print(((torch.tensor(out1) - out2) < 1e-7).all())\n",
    "\n",
    "## BACKWARD TESTING\n",
    "loss = out2.sum()\n",
    "loss.backward()\n",
    "\n",
    "dx = l1.backward(np.ones((5, 15)), back)\n",
    "print(((torch.tensor(l1.weight.grad) - l2.weight.grad.T) < 1e-6).all())\n",
    "\n",
    "print(((torch.tensor(dx) - x2.grad) < 1e-6).all()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0.,x), {'mask': x > 0}\n",
    "    \n",
    "    def backward(self, dout, back):\n",
    "        return dout*back['mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, p = 0.5):\n",
    "        self.p = p \n",
    "        self.mode = 'Training'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mode == 'Infer': self.p = 1\n",
    "        mask = np.random.random(x.shape) > self.p\n",
    "        return x*mask, {'mask': mask}\n",
    "    \n",
    "    def backward(self, dout, back):\n",
    "        return mask*dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class BatchNorm1D():\n",
    "    def __init__(self, num_outs, beta_m = 0.9):\n",
    "        self.gamma = Parameters(np.ones(num_outs))\n",
    "        self.beta = Parameters(np.zeros(num_outs))\n",
    "        \n",
    "        self.wma_mean = np.zeros_like(self.gamma)\n",
    "        self.wma_var = np.zeros_like(self.beta)\n",
    "        self.beta_m = beta_m\n",
    "        \n",
    "        self.mode = 'Training'\n",
    "        \n",
    "    def update_stats(self, name, value):\n",
    "        stat = getattr(self, name)\n",
    "        setattr(self, name, self.beta_m*stat + (1 - self.beta_m)*value)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mode == 'Infer': \n",
    "            mean, std = self.wma_mean, self.wma_var\n",
    "        elif self.mode == 'Training':\n",
    "            mean, std = np.mean(x, axis= 0), np.std(x, axis = 0)\n",
    "        else: raise NameError('Invalid mode {}'.format(self.mode))\n",
    "            \n",
    "        update_stats('wma_mean', mean)\n",
    "        update_stats('wma_std', std)\n",
    "        norm_x = (x - mean)/std\n",
    "        out = self.gamma.data * norm_x + self.beta.data\n",
    "        return out, {'mean': mean, 'var': var, 'x': x, 'norm_x': norm_x}\n",
    "    \n",
    "    def backward(self, dout, back):\n",
    "        batch_size = dout.shape[0]\n",
    "        mean, var, x, norm_x = back['mean'], back['var'], back['x'], back['norm_x']\n",
    "        self.beta.grad = np.sum(dout, axis= 0)\n",
    "        self.gamma.grad = np.sum(dout*norm_x, axis= 0)\n",
    "        \n",
    "        dnorm_x = dout*self.gamma.data\n",
    "        dx_sub_mean1 = dnorm_x/std\n",
    "        dstd_invert = np.sum(dnorm*(x - mean), axis= 0)\n",
    "        dstd = (-1/(std**2))*dstd\n",
    "        \n",
    "        dvar = dstd*np.sqrt(std)/2\n",
    "        dx_sub_mean_square = np.tile(dvar, (batch_size, 1))\n",
    "        dx_sub_mean2 = 2 * dx_sub_mean_square * (x - mean)\n",
    "        \n",
    "        dx_sub_mean = dx_sub_mean1 + dx_sub_mean2\n",
    "        dmean = -dx_sub_mean\n",
    "        dx1 = dx_sub_mean\n",
    "        dx2 = np.tile(dmean, (batch_size, 1))/batch_size\n",
    "        \n",
    "        return dx1 + dx2\n",
    "    \n",
    "    @property\n",
    "    def params(cls):\n",
    "        return (getattr(cls, i) for i in cls.__dict__.keys() if isinstance(getattr(cls, i), Parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet():\n",
    "    def __init__(self, num_ins, num_hids, num_outs):\n",
    "        ## better to create a sequence here\n",
    "        self.layers = [Linear(num_ins, num_hids), Relu(), Linear(num_hids, num_outs)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        backs = [], []\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out, back = layer.forward(out)\n",
    "            backs.append(back)\n",
    "        \n",
    "        return out, backs\n",
    "    \n",
    "    def backward(self, dout, backs):\n",
    "        for back, layer in zip(backs, layers):\n",
    "            dout = layer.backward(dout, back)\n",
    "            \n",
    "    def training_mode():\n",
    "        pass\n",
    "    \n",
    "    def infer_mode():\n",
    "        pass\n",
    "            \n",
    "    @property\n",
    "    def params(self):\n",
    "        return chain(*[layer.params for layer in self.layers if hasattr(layer, 'params')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp/np.sum(x_exp, axis= 1, keepdims= True)\n",
    "\n",
    "def one_hot(x,num_cls = None):\n",
    "    batch_size = x.shape[0]\n",
    "    num_cls = num_cls if num_cls is not None else int(np.max(x))\n",
    "    one = np.zeros((batch_size, num_cls))\n",
    "    one[range(batch_size), x] = 1\n",
    "    return one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "    def forward(self, x, target):\n",
    "        \n",
    "        def logsumexp(x):\n",
    "            xmax = np.max(x, axis= -1, keepdims= True)\n",
    "            return xmax + np.log(np.sum(np.exp(x - xmax), axis= -1, keepdims= True))\n",
    "        \n",
    "        def logsoftmax(x):\n",
    "            return x - logsumexp(x)\n",
    "        \n",
    "        logp = logsoftmax(x)\n",
    "        loss = -np.mean(logp[range(len(target)), target])\n",
    "        \n",
    "        return loss, {'x': x, 'target': target}\n",
    "    \n",
    "    def backward(self, backs):\n",
    "        x, target = back['x'], back['target']\n",
    "        return (softmax(x) - one_hot(target))/x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, params, lr = 0.01, weight_decay = 0.):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = np.zeros_like(param.grad)\n",
    "        \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data = param.data - self.lr*(param.grad + 2*self.weight_decay*param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
