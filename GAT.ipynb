{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import dgl\n",
    "from dgl import function as fn\n",
    "from dgl.base import DGLError\n",
    "from dgl.utils import expand_as_pair, check_eq_shape\n",
    "from dgl.data import citation_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n"
     ]
    }
   ],
   "source": [
    "data = citation_graph.load_cora()\n",
    "features = data.features\n",
    "labels = torch.LongTensor(data.labels)\n",
    "train_mask = torch.BoolTensor(data.train_mask)\n",
    "val_mask = torch.BoolTensor(data.val_mask)\n",
    "g = dgl.from_networkx(data.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATConv(nn.Module):\n",
    "    def __init__(self, g, num_ins, num_outs,activation = True):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(num_ins, num_outs, bias = True)\n",
    "        self.fc2 = nn.Linear(num_outs * 2, 1)\n",
    "        self.activation = activation\n",
    "        self.g = g\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def message_passing(self, edges):  \n",
    "        return {'m': edges.src['z'], 's': edges.data['s']}\n",
    "        \n",
    "    def reduce_function(self, nodes):\n",
    "        return {'out': torch.sum(nodes.mailbox['m']*nodes.mailbox['s'], dim = 1)}\n",
    "    \n",
    "    def edge_attention(self, edges):\n",
    "        eatt = F.relu(self.fc2(torch.cat([edges.src['z'], edges.dst['z']], dim = -1)))\n",
    "        return {'s': F.softmax(eatt, -1)}\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        self.g.ndata.update({'z': self.fc(inputs)})        \n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        self.g.update_all(self.message_passing, self.reduce_function)\n",
    "        if self.activation: return F.relu(self.g.ndata['out'])\n",
    "        else: return self.g.ndata['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadGAT(nn.Module):\n",
    "    def __init__(self, g, num_ins, num_outs, concat = True, k = 8, activation = True):\n",
    "        super().__init__()\n",
    "        self.concat = concat\n",
    "        self.activation = activation\n",
    "        if concat:\n",
    "            assert num_outs % k == 0, 'invalid num outs'\n",
    "            self.gconvs =  nn.ModuleList([GATConv(g, num_ins, num_outs//k) for i in range(k)])\n",
    "        else: \n",
    "            self.gconvs =  nn.ModuleList([GATConv(g, num_ins, num_outs, activation= False) for i in range(k)])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if self.concat:\n",
    "            return torch.cat([gconv(inputs) for gconv in self.gconvs], dim = 1)\n",
    "        else: \n",
    "            out = torch.stack([gconv(inputs) for gconv in self.gconvs], dim = 0)\n",
    "            out = torch.mean(out, dim = 0)\n",
    "            return F.relu(out) if self.activation else out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phan.huy.hoang/anaconda3/envs/tambnm36_env/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.0811610221862793\n",
      "epoch: 1, loss: 1.8620535135269165\n",
      "epoch: 2, loss: 1.8042323589324951\n",
      "epoch: 3, loss: 1.7786604166030884\n",
      "epoch: 4, loss: 1.742370843887329\n",
      "epoch: 5, loss: 1.6985654830932617\n",
      "epoch: 6, loss: 1.6517014503479004\n",
      "epoch: 7, loss: 1.6036046743392944\n",
      "epoch: 8, loss: 1.5528745651245117\n",
      "epoch: 9, loss: 1.501745581626892\n",
      "epoch: 10, loss: 1.4532300233840942\n",
      "epoch: 11, loss: 1.4100029468536377\n",
      "epoch: 12, loss: 1.3730733394622803\n",
      "epoch: 13, loss: 1.340494990348816\n",
      "epoch: 14, loss: 1.3077342510223389\n",
      "epoch: 15, loss: 1.2711265087127686\n",
      "epoch: 16, loss: 1.2322332859039307\n",
      "epoch: 17, loss: 1.1940268278121948\n",
      "epoch: 18, loss: 1.1575514078140259\n",
      "epoch: 19, loss: 1.1224384307861328\n",
      "epoch: 20, loss: 1.088490605354309\n",
      "epoch: 21, loss: 1.055169939994812\n",
      "epoch: 22, loss: 1.0215721130371094\n",
      "epoch: 23, loss: 0.9876906275749207\n",
      "epoch: 24, loss: 0.9543854594230652\n",
      "epoch: 25, loss: 0.9222344756126404\n",
      "epoch: 26, loss: 0.8913723826408386\n",
      "epoch: 27, loss: 0.8616583943367004\n",
      "epoch: 28, loss: 0.8325845003128052\n",
      "epoch: 29, loss: 0.8038768172264099\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(MultiheadGAT(g, 1433, 480), MultiheadGAT(g, 480, 7, concat= False, activation=False))\n",
    "optimizer = optim.Adam(net.parameters(), lr = 1e-3)\n",
    "\n",
    "for epoch in range(30):\n",
    "    logits = net(features)\n",
    "    logp = F.log_softmax(logits)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print('epoch: {0}, loss: {1}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(features, mask):\n",
    "    num = torch.tensor((features[mask]).shape[0], dtype = torch.float)\n",
    "    logits = net(features)\n",
    "    return torch.sum(torch.max(logits, dim = 1)[1][mask] == labels[mask])/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7360)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(features, val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3071, -0.2970,  0.4974],\n",
       "         [-0.3593,  0.2232,  0.3549],\n",
       "         [-0.1576, -0.5640,  0.1042],\n",
       "         [-0.5513,  0.1275, -0.0775]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0260,  0.3171, -0.0312, -0.0210], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.4282, -0.0090,  0.2219, -0.3619],\n",
       "         [ 0.2819,  0.0856,  0.0566, -0.4666]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1567,  0.4183], requires_grad=True)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Sequential(nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 2)), nn.ReLU())\n",
    "list(a.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tambnm36",
   "language": "python",
   "name": "conda_tambnm36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
